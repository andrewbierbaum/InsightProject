{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o41.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): com.univocity.parsers.common.TextParsingException: java.lang.IllegalStateException - Error reading from input\nParser Configuration: CsvParserSettings:\n\tAuto configuration enabled=true\n\tAutodetect column delimiter=false\n\tAutodetect quotes=false\n\tColumn reordering enabled=true\n\tEmpty value=null\n\tEscape unquoted values=false\n\tHeader extraction enabled=null\n\tHeaders=null\n\tIgnore leading whitespaces=false\n\tIgnore trailing whitespaces=false\n\tInput buffer size=128\n\tInput reading on separate thread=false\n\tKeep escape sequences=false\n\tKeep quotes=false\n\tLength of content displayed on error=-1\n\tLine separator detection enabled=false\n\tMaximum number of characters per column=-1\n\tMaximum number of columns=20480\n\tNormalize escaped line separators=true\n\tNull value=\n\tNumber of records to read=all\n\tProcessor=none\n\tRestricting data in exceptions=false\n\tRowProcessor error handler=null\n\tSelected fields=none\n\tSkip bits as whitespace=true\n\tSkip empty lines=true\n\tUnescaped quote handling=STOP_AT_DELIMITERFormat configuration:\n\tCsvFormat:\n\t\tComment character=\\0\n\t\tField delimiter=,\n\t\tLine separator (normalized)=\\n\n\t\tLine separator sequence=\\n\n\t\tQuote character=\"\n\t\tQuote escape character=\"\n\t\tQuote escape escape character=null\nInternal state when error was thrown: line=0, column=0, record=0\n\tat com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:368)\n\tat com.univocity.parsers.common.AbstractParser.beginParsing(AbstractParser.java:247)\n\tat com.univocity.parsers.common.AbstractParser.beginParsing(AbstractParser.java:722)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anon$1.<init>(UnivocityParser.scala:265)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$.convertStream(UnivocityParser.scala:264)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$.tokenizeStream(UnivocityParser.scala:239)\n\tat org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$$anonfun$8.apply(CSVDataSource.scala:223)\n\tat org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$$anonfun$8.apply(CSVDataSource.scala:221)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Error reading from input\n\tat com.univocity.parsers.common.input.DefaultCharInputReader.reloadBuffer(DefaultCharInputReader.java:80)\n\tat com.univocity.parsers.common.input.AbstractCharInputReader.updateBuffer(AbstractCharInputReader.java:191)\n\tat com.univocity.parsers.common.input.AbstractCharInputReader.start(AbstractCharInputReader.java:172)\n\tat com.univocity.parsers.common.AbstractParser.beginParsing(AbstractParser.java:245)\n\t... 30 more\nCaused by: java.io.IOException: not a gzip file\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.processBasicHeader(BuiltInGzipDecompressor.java:496)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.executeHeaderState(BuiltInGzipDecompressor.java:257)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:186)\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:91)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)\n\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n\tat com.univocity.parsers.common.input.DefaultCharInputReader.reloadBuffer(DefaultCharInputReader.java:78)\n\t... 33 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1358)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1331)\n\tat org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$.infer(CSVDataSource.scala:227)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:201)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:392)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:596)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.univocity.parsers.common.TextParsingException: java.lang.IllegalStateException - Error reading from input\nParser Configuration: CsvParserSettings:\n\tAuto configuration enabled=true\n\tAutodetect column delimiter=false\n\tAutodetect quotes=false\n\tColumn reordering enabled=true\n\tEmpty value=null\n\tEscape unquoted values=false\n\tHeader extraction enabled=null\n\tHeaders=null\n\tIgnore leading whitespaces=false\n\tIgnore trailing whitespaces=false\n\tInput buffer size=128\n\tInput reading on separate thread=false\n\tKeep escape sequences=false\n\tKeep quotes=false\n\tLength of content displayed on error=-1\n\tLine separator detection enabled=false\n\tMaximum number of characters per column=-1\n\tMaximum number of columns=20480\n\tNormalize escaped line separators=true\n\tNull value=\n\tNumber of records to read=all\n\tProcessor=none\n\tRestricting data in exceptions=false\n\tRowProcessor error handler=null\n\tSelected fields=none\n\tSkip bits as whitespace=true\n\tSkip empty lines=true\n\tUnescaped quote handling=STOP_AT_DELIMITERFormat configuration:\n\tCsvFormat:\n\t\tComment character=\\0\n\t\tField delimiter=,\n\t\tLine separator (normalized)=\\n\n\t\tLine separator sequence=\\n\n\t\tQuote character=\"\n\t\tQuote escape character=\"\n\t\tQuote escape escape character=null\nInternal state when error was thrown: line=0, column=0, record=0\n\tat com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:368)\n\tat com.univocity.parsers.common.AbstractParser.beginParsing(AbstractParser.java:247)\n\tat com.univocity.parsers.common.AbstractParser.beginParsing(AbstractParser.java:722)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anon$1.<init>(UnivocityParser.scala:265)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$.convertStream(UnivocityParser.scala:264)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$.tokenizeStream(UnivocityParser.scala:239)\n\tat org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$$anonfun$8.apply(CSVDataSource.scala:223)\n\tat org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$$anonfun$8.apply(CSVDataSource.scala:221)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalStateException: Error reading from input\n\tat com.univocity.parsers.common.input.DefaultCharInputReader.reloadBuffer(DefaultCharInputReader.java:80)\n\tat com.univocity.parsers.common.input.AbstractCharInputReader.updateBuffer(AbstractCharInputReader.java:191)\n\tat com.univocity.parsers.common.input.AbstractCharInputReader.start(AbstractCharInputReader.java:172)\n\tat com.univocity.parsers.common.AbstractParser.beginParsing(AbstractParser.java:245)\n\t... 30 more\nCaused by: java.io.IOException: not a gzip file\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.processBasicHeader(BuiltInGzipDecompressor.java:496)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.executeHeaderState(BuiltInGzipDecompressor.java:257)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:186)\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:91)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)\n\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n\tat com.univocity.parsers.common.input.DefaultCharInputReader.reloadBuffer(DefaultCharInputReader.java:78)\n\t... 33 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a3d7b24d9dac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reddit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://andrew-bierbaum-insight-test-dataset/Reddit/Reddit_Comments_2008-000*.csv.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmultiLine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o41.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): com.univocity.parsers.common.TextParsingException: java.lang.IllegalStateException - Error reading from input\nParser Configuration: CsvParserSettings:\n\tAuto configuration enabled=true\n\tAutodetect column delimiter=false\n\tAutodetect quotes=false\n\tColumn reordering enabled=true\n\tEmpty value=null\n\tEscape unquoted values=false\n\tHeader extraction enabled=null\n\tHeaders=null\n\tIgnore leading whitespaces=false\n\tIgnore trailing whitespaces=false\n\tInput buffer size=128\n\tInput reading on separate thread=false\n\tKeep escape sequences=false\n\tKeep quotes=false\n\tLength of content displayed on error=-1\n\tLine separator detection enabled=false\n\tMaximum number of characters per column=-1\n\tMaximum number of columns=20480\n\tNormalize escaped line separators=true\n\tNull value=\n\tNumber of records to read=all\n\tProcessor=none\n\tRestricting data in exceptions=false\n\tRowProcessor error handler=null\n\tSelected fields=none\n\tSkip bits as whitespace=true\n\tSkip empty lines=true\n\tUnescaped quote handling=STOP_AT_DELIMITERFormat configuration:\n\tCsvFormat:\n\t\tComment character=\\0\n\t\tField delimiter=,\n\t\tLine separator (normalized)=\\n\n\t\tLine separator sequence=\\n\n\t\tQuote character=\"\n\t\tQuote escape character=\"\n\t\tQuote escape escape character=null\nInternal state when error was thrown: line=0, column=0, record=0\n\tat com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:368)\n\tat com.univocity.parsers.common.AbstractParser.beginParsing(AbstractParser.java:247)\n\tat com.univocity.parsers.common.AbstractParser.beginParsing(AbstractParser.java:722)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anon$1.<init>(UnivocityParser.scala:265)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$.convertStream(UnivocityParser.scala:264)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$.tokenizeStream(UnivocityParser.scala:239)\n\tat org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$$anonfun$8.apply(CSVDataSource.scala:223)\n\tat org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$$anonfun$8.apply(CSVDataSource.scala:221)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Error reading from input\n\tat com.univocity.parsers.common.input.DefaultCharInputReader.reloadBuffer(DefaultCharInputReader.java:80)\n\tat com.univocity.parsers.common.input.AbstractCharInputReader.updateBuffer(AbstractCharInputReader.java:191)\n\tat com.univocity.parsers.common.input.AbstractCharInputReader.start(AbstractCharInputReader.java:172)\n\tat com.univocity.parsers.common.AbstractParser.beginParsing(AbstractParser.java:245)\n\t... 30 more\nCaused by: java.io.IOException: not a gzip file\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.processBasicHeader(BuiltInGzipDecompressor.java:496)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.executeHeaderState(BuiltInGzipDecompressor.java:257)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:186)\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:91)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)\n\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n\tat com.univocity.parsers.common.input.DefaultCharInputReader.reloadBuffer(DefaultCharInputReader.java:78)\n\t... 33 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1358)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1331)\n\tat org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$.infer(CSVDataSource.scala:227)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:201)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:392)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:596)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.univocity.parsers.common.TextParsingException: java.lang.IllegalStateException - Error reading from input\nParser Configuration: CsvParserSettings:\n\tAuto configuration enabled=true\n\tAutodetect column delimiter=false\n\tAutodetect quotes=false\n\tColumn reordering enabled=true\n\tEmpty value=null\n\tEscape unquoted values=false\n\tHeader extraction enabled=null\n\tHeaders=null\n\tIgnore leading whitespaces=false\n\tIgnore trailing whitespaces=false\n\tInput buffer size=128\n\tInput reading on separate thread=false\n\tKeep escape sequences=false\n\tKeep quotes=false\n\tLength of content displayed on error=-1\n\tLine separator detection enabled=false\n\tMaximum number of characters per column=-1\n\tMaximum number of columns=20480\n\tNormalize escaped line separators=true\n\tNull value=\n\tNumber of records to read=all\n\tProcessor=none\n\tRestricting data in exceptions=false\n\tRowProcessor error handler=null\n\tSelected fields=none\n\tSkip bits as whitespace=true\n\tSkip empty lines=true\n\tUnescaped quote handling=STOP_AT_DELIMITERFormat configuration:\n\tCsvFormat:\n\t\tComment character=\\0\n\t\tField delimiter=,\n\t\tLine separator (normalized)=\\n\n\t\tLine separator sequence=\\n\n\t\tQuote character=\"\n\t\tQuote escape character=\"\n\t\tQuote escape escape character=null\nInternal state when error was thrown: line=0, column=0, record=0\n\tat com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:368)\n\tat com.univocity.parsers.common.AbstractParser.beginParsing(AbstractParser.java:247)\n\tat com.univocity.parsers.common.AbstractParser.beginParsing(AbstractParser.java:722)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anon$1.<init>(UnivocityParser.scala:265)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$.convertStream(UnivocityParser.scala:264)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$.tokenizeStream(UnivocityParser.scala:239)\n\tat org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$$anonfun$8.apply(CSVDataSource.scala:223)\n\tat org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$$anonfun$8.apply(CSVDataSource.scala:221)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1358)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalStateException: Error reading from input\n\tat com.univocity.parsers.common.input.DefaultCharInputReader.reloadBuffer(DefaultCharInputReader.java:80)\n\tat com.univocity.parsers.common.input.AbstractCharInputReader.updateBuffer(AbstractCharInputReader.java:191)\n\tat com.univocity.parsers.common.input.AbstractCharInputReader.start(AbstractCharInputReader.java:172)\n\tat com.univocity.parsers.common.AbstractParser.beginParsing(AbstractParser.java:245)\n\t... 30 more\nCaused by: java.io.IOException: not a gzip file\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.processBasicHeader(BuiltInGzipDecompressor.java:496)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.executeHeaderState(BuiltInGzipDecompressor.java:257)\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:186)\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:91)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)\n\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n\tat com.univocity.parsers.common.input.DefaultCharInputReader.reloadBuffer(DefaultCharInputReader.java:78)\n\t... 33 more\n"
     ]
    }
   ],
   "source": [
    "#sparting spark and reading Reddit\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Reddit\").getOrCreate()\n",
    "df = None\n",
    "df = spark.read.csv(\"s3a://andrew-bierbaum-insight-test-dataset/Reddit/Reddit_Comments_2008-000*.csv.gz\", header=True,multiLine=True, escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql.caseSensitive = False\n",
    "#sqlContext.sql(\"set spark.sql.caseSensitive=false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the start of the data and format\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert spark data to be readable using sql queries\n",
    "df.createOrReplaceTempView(\"Reddit\")\n",
    "microsoft_results = spark.sql(\"SELECT created_utc, body FROM Reddit WHERE body LIKE '%microsoft%'\")\n",
    "facebook_results = spark.sql(\"SELECT created_utc, body FROM Reddit WHERE body LIKE '%facebook%'\")\n",
    "amazon_results = spark.sql(\"SELECT created_utc, body FROM Reddit WHERE body LIKE '%amazon%'\")\n",
    "google_results = spark.sql(\"SELECT created_utc, body FROM Reddit WHERE body LIKE '%google%'\")\n",
    "microsoft_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect, convert dates to datetime format for later graphing, and sort data\n",
    "from datetime import datetime\n",
    "python_microsoft_results = microsoft_results.collect()\n",
    "python_microsoft_results_cleaned = [(datetime.fromtimestamp(float(i)),body.encode('ascii',errors='ignore')) for i, body in python_microsoft_results]\n",
    "python_microsoft_results_cleaned.sort()\n",
    "\n",
    "#b = bytes('pythön', encoding='utf-8')\n",
    "#print(str(b, encoding='ascii', errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation steps\n",
    "\n",
    "#print python_microsoft_results_cleaned\n",
    "#print python_microsoft_results_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort, number, and then graph the data\n",
    "%matplotlib inline\n",
    "import numpy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "count = numpy.arange(len(python_microsoft_results_cleaned))\n",
    "#Date_Data = matplotlib.dates.datestr2num(clean_python_results_utc)\n",
    "Date_Data = []\n",
    "Body_Data = []\n",
    "for date, body in python_microsoft_results_cleaned:\n",
    "    Date_Data.append(date)\n",
    "    Body_Data.append(body)\n",
    "matplotlib.pyplot.plot_date(Date_Data,count,xdate=True, drawstyle = 'steps-pre', linestyle = 'solid' )\n",
    "matplotlib.pyplot.ylabel('Microsoft Mentions')\n",
    "matplotlib.pyplot.title('Reddit 2006 Microsoft mentions')\n",
    "plt.show()\n",
    "plt.savefig('Reddit2006.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "import pandas\n",
    "\n",
    "# with open('dash.csv', 'w') as csvfile:   #, newline=''\n",
    "#     #csv.writer(csvfile, delimiter=' ', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     writer.writerows(\"count\",\"Date_Data\")\n",
    "\n",
    "#zipped_Data = zip(Date_Data, body)\n",
    "#pandas_df = pandas.DataFrame(zipped_Data)\n",
    "pandas_df = pandas.DataFrame({'date':Date_Data,'body':Body_Data})\n",
    "pandas_df.head()\n",
    "pandas_df.to_csv(\"dash.csv\")\n",
    "\n",
    "# percentile_list = pd.DataFrame(\n",
    "#     {'lst1Title': lst1,\n",
    "#      'lst2Title': lst2,\n",
    "#      'lst3Title': lst3\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import pandas\n",
    "\n",
    "# pandas_df = pandas.DataFrame(python_results_timestamp, columns = ['count, timestamp'])\n",
    "# pandas_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dash\n",
    "# import dash_core_components as dcc\n",
    "# import dash_html_components as html\n",
    "\n",
    "# external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "# app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "# app.layout = html.Div(children=[\n",
    "#     html.H1(children='Reddit 2006'),\n",
    "\n",
    "#     html.Div(children='''\n",
    "#         Dash: A web application framework for Python.\n",
    "#     '''),\n",
    "\n",
    "#     dcc.Graph(\n",
    "#         id='Reddit 2006 python mentions',\n",
    "#         figure={\n",
    "#             'data': [\n",
    "#                 {'x': Date_Data, 'y': count, 'type': 'scatter', 'name': 'SF'}\n",
    "#             ],\n",
    "#             'layout': {\n",
    "#                 'title': 'Dash Data Visualization'\n",
    "#             }\n",
    "#         }\n",
    "#     )\n",
    "# ])\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run_server(debug=True, port = 9990, host ='0.0.0.0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
